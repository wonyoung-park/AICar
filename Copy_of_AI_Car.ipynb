{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of AI Car",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "ZZ024N7vUn4n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Outline\n",
        "\n",
        "Here we will train a fully-connected network to classify hand-written digits from images using the MNIST dataset\n",
        "\n",
        "The network will take 28x28 pixel images and map them to digit categories {0, 1, 2, ..., 9}. "
      ]
    },
    {
      "metadata": {
        "id": "psOOYZtbUn4o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b5d82fc9-d0f9-4f71-e221-b27ecb3ae355"
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "accelerator = 'cu90' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x59cc0000 @  0x7f5824c941c4 0x46d6a4 0x5fcbcc 0x4c494d 0x54f3c4 0x553aaf 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54e4c8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jZBRFUmdiqeq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "171636e0-6156-4dd6-edba-1e3c7e8afb28"
      },
      "cell_type": "code",
      "source": [
        "!pip freeze | grep torch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch==0.4.1\n",
            "torchvision==0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OvgKf8RrMNh7",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "60360e35-1e47-42b9-fce2-7a78862651d6"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-15819320-57a9-4dd8-87ea-e8ee064cf385\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-15819320-57a9-4dd8-87ea-e8ee064cf385\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving steering.csv to steering.csv\n",
            "User uploaded file \"steering.csv\" with length 195642 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lH1fvCyfVz3w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!unzip Final_Four18-07-27-11_47_31.zip #unzip file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XnGZYAk9W4Om",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cd Final_Four18-07-27-11_47_31/\n",
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "glWjQgMxW_Ka",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!rm /content/Final_Four18-07-27-11_47_31/steering.csv\n",
        "#!rm /content/steering*\n",
        "!mv /content/steering.csv /content/Final_Four18-07-27-11_47_31"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qfz4PdJjai-u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ls -l /content/Final_Four/DATA/Final_Four18-07-27-11\\:47\\:31/steering.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v722sk3GMrcm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3d5372d4-fc1f-4c6a-fe7a-3aa29eb9b2d7"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1PBJHqLd4-ue",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.cuda.set_device(0)\n",
        "# On device 0\n",
        "with torch.cuda.device(1):\n",
        "    print(\"Inside device is 1\")    \n",
        "    # On device 1\n",
        "print(\"Outside is still 0\")\n",
        "# On device 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UXzz9sswUn4u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load the Dataset (MNIST)\n",
        "\n",
        "\n",
        "We can use some PyTorch DataLoader utilities for this.  "
      ]
    },
    {
      "metadata": {
        "id": "2b86nRdFcWTs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!head /content/Final_Four18-07-27-11_47_31/steering.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "msPcM-4UaAPW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "c3ab1817-5309-4c1a-dfca-4a637c7ac9ce"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "import csv\n",
        "\n",
        "\n",
        "#from torch.utils.data.sampler import SubsetRandomSampler\n",
        "#from torch.utils.data import random_split\n",
        "\n",
        "from torch import randperm\n",
        "from torch.utils import _accumulate\n",
        "\n",
        "def random_split(dataset, lengths):\n",
        "    \"\"\"\n",
        "    Randomly split a dataset into non-overlapping new datasets of given lengths.\n",
        "\n",
        "    Arguments:\n",
        "        dataset (Dataset): Dataset to be split\n",
        "        lengths (sequence): lengths of splits to be produced\n",
        "    \"\"\"\n",
        "    if sum(lengths) != len(dataset):\n",
        "        raise ValueError(\"Sum of input lengths does not equal the length of the input dataset!\")\n",
        "\n",
        "    indices = randperm(sum(lengths))\n",
        "    return [Subset(dataset, indices[offset - length:offset]) for offset, length in zip(_accumulate(lengths), lengths)]\n",
        "\n",
        "\n",
        "  \n",
        "#Type your own directory where you stored the data, fill in the blank!\n",
        "FOLDER_DATASET = \"/content/Final_Four18-07-27-11_47_31/\"\n",
        "# plt.ion()\n",
        "\n",
        "class DriveData(Dataset):\n",
        "    __xs = []\n",
        "    __ys = []\n",
        "\n",
        "    def __init__(self, folder_dataset, transform=None):\n",
        "        self.transform = transform\n",
        "        # Open and load text file including the whole training data\n",
        "        with open(folder_dataset + \"steering.csv\") as f:\n",
        "            reader = csv.reader(f)\n",
        "            for line in reader:\n",
        "                # Image path\n",
        "                self.__xs.append(line[2])        \n",
        "                # Steering wheel label\n",
        "                self.__ys.append(np.float(line[0]))\n",
        "\n",
        "    # Override to give PyTorch access to any image on the dataset\n",
        "    def __getitem__(self, index):\n",
        "        img = Image.open(self.__xs[index])\n",
        "        img = img.convert('RGB')\n",
        "        label = torch.from_numpy(np.asarray((self.__ys[index]-6000)/2000).reshape(1)).float()\n",
        "        # random flip the images and reverse the steering command, DATA AUGMENTATION\n",
        "        if random.randint(0, 1) == 0:\n",
        "            img = flip(img)\n",
        "            label = -label\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        else:\n",
        "            # Convert image and label to torch tensors\n",
        "            img = np.transpose(np.asarray(img),(2,0,1)) #size [120,160,3]--->[3,120,160]\n",
        "            img = torch.from_numpy(img/255.0) #size [3,120,160]\n",
        "            \n",
        "        img = img[:,60:120,:]\n",
        "\n",
        "        return img, label\n",
        "\n",
        "    # Override to give PyTorch size of dataset\n",
        "    def __len__(self):\n",
        "        return len(self.__xs)\n",
        "\n",
        "# Please vist https://pytorch.org/docs/stable/torchvision/transforms.html to see different kinds of transformations\n",
        "preprocessing = transforms.Compose([\n",
        "   transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "\n",
        "flip = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=1),\n",
        "])\n",
        "\n",
        "\n",
        "dset_train = DriveData(FOLDER_DATASET, transform=preprocessing)\n",
        "\n",
        "train_size = int(0.8 * len(dset_train))\n",
        "test_size = len(dset_train) - train_size\n",
        "train_dataset, test_dataset = random_split(dset_train, [train_size, test_size])\n",
        "\n",
        "#train_loader = DataLoader(dset_train, batch_size=5, shuffle=True, num_workers=1) #fill in the blank\n",
        "#test_loader = DataLoader(dset_test, batch_size=5, shuffle=True, num_workers=1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-115-60d39a28446b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandperm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_accumulate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrandom_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name '_accumulate'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "sHOjfs-JQF64",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ny7xoRhB7OQI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7b8090e9-80c7-4268-80a5-3971f9e46af6"
      },
      "cell_type": "code",
      "source": [
        "!ls Final_Four18-07-27-11:47:31/steering.csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access 'Final_Four18-07-27-11:47:31/steering.csv': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E2VxOAaGUn4x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Display some images"
      ]
    },
    {
      "metadata": {
        "id": "JqeCmqEFUn40",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# matplotlib and stuff\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def plt_style(c='k'):\n",
        "    \"\"\"\n",
        "    Set plotting style for bright (``c = 'w'``) or dark (``c = 'k'``) backgrounds\n",
        "\n",
        "    :param c: color, can be set to ``'w'`` or ``'k'`` (which is the default)\n",
        "    :type c: str\n",
        "    \"\"\"\n",
        "    import matplotlib as mpl\n",
        "    from matplotlib import rc\n",
        "\n",
        "    # Reset previous configuration\n",
        "    mpl.rcParams.update(mpl.rcParamsDefault)\n",
        "    #%matplotlib inline  # not from script\n",
        "    get_ipython().run_line_magic('matplotlib', 'inline')\n",
        "\n",
        "    # configuration for bright background\n",
        "    if c == 'w':\n",
        "        plt.style.use('bmh')\n",
        "\n",
        "    # configurations for dark background\n",
        "    if c == 'k':\n",
        "        plt.style.use(['dark_background', 'bmh'])\n",
        "\n",
        "    # remove background colour, set figure size\n",
        "    rc('figure', figsize=(16, 8), max_open_warning=False)\n",
        "    rc('axes', facecolor='none')\n",
        "    rc('nbagg', transparent=False)\n",
        "\n",
        "\n",
        "def plt_interactive(c='k'):\n",
        "    from matplotlib import rc\n",
        "    import matplotlib as mpl\n",
        "    mpl.rcParams.update(mpl.rcParamsDefault)\n",
        "    get_ipython().run_line_magic('matplotlib', 'notebook')\n",
        "    plt.rc('figure', figsize=(160, 60), facecolor=c)\n",
        "    rc('nbagg', transparent=False)\n",
        "    # configuration for bright background\n",
        "    if c == 'w':\n",
        "        plt.style.use('bmh')\n",
        "\n",
        "    # configurations for dark background\n",
        "    if c == 'k':\n",
        "        plt.style.use(['dark_background', 'bmh'])\n",
        "    rc('axes', facecolor='none')\n",
        "\n",
        "plt_style()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-8ql8sWjf-DH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip install Pillow==4.0.0\n",
        "#!pip install PIL\n",
        "#!pip install image\n",
        "from PIL import Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GiPJUXSKUn43",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    imgs, steering_angle = next(iter(train_loader))\n",
        "    #print('Batch shape:',imgs.size())\n",
        "    #print(steering_angle)\n",
        "\n",
        "    #plt.imshow(np.transpose(imgs.numpy()[0,:,:,:],(1,2,0)))\n",
        "    #plt.show()\n",
        "    #plt.imshow(np.transpose(imgs.numpy()[-1,:,:,:],(1,2,0)))\n",
        "    #plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zhD3XIzBUn45",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Create the model class"
      ]
    },
    {
      "metadata": {
        "id": "-8zwxRs6Un46",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super(ConvNet, self).__init__()\n",
        "    self.layer1 = nn.Sequential(\n",
        "      nn.Conv2d(3, 9, kernel_size=5, stride=1, padding=0),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "    self.layer2 = nn.Sequential(\n",
        "      nn.Conv2d(9, 27, kernel_size=5, stride=1, padding=0),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "    self.layer3 = nn.Sequential(\n",
        "      nn.Conv2d(27, 71, kernel_size=4, stride=1, padding=0),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "    self.layer4 = nn.Sequential(\n",
        "      nn.Conv2d(71, 140, kernel_size=3, stride=1, padding=0),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "    #self.layer5 = nn.Sequential(\n",
        "     #  nn.Conv2d(140,160, kernel_size=3, stride=1, padding=0),\n",
        "     #  nn.ReLU(),\n",
        "     #  nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "\n",
        "    self.fc1 = nn.Sequential(\n",
        "        nn.Linear(1*7*140, 100),\n",
        "        nn.ReLU())\n",
        "    self.fc2 = nn.Sequential(\n",
        "        nn.Linear(100, 50),\n",
        "        nn.ReLU())\n",
        "    self.fc3 = nn.Sequential(\n",
        "        nn.Linear(50, 10),\n",
        "        nn.ReLU())\n",
        "    self.fc4 = nn.Sequential(\n",
        "        nn.Linear(10, num_classes))\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.layer1(x)\n",
        "    out = self.layer2(out)\n",
        "    out = self.layer3(out)\n",
        "    out = self.layer4(out)\n",
        "    #out = self.layer5(out)\n",
        "    out = out.reshape(out.size(0), -1)\n",
        "    out = self.fc1(out)\n",
        "    out = self.fc2(out)\n",
        "    out = self.fc3(out)\n",
        "    out = self.fc4(out)\n",
        "\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YHXavVmRUn47",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Define Training Loop\n",
        "\n",
        "* Loop batches of samples in the training set\n",
        "* Run each batch through the model (forward pass)\n",
        "* Compute the loss\n",
        "* Compute the gradients with respect to model parameters (backward pass)\n",
        "* Update the parameters"
      ]
    },
    {
      "metadata": {
        "id": "pn6QDQPzUn48",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define training loop\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # Move tensors to the configured device\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        ########### Forward pass #############\n",
        "        y = model(data)\n",
        "        ##########calculate the loss##########\n",
        "\n",
        "        current_loss = F.mse_loss(y, target)\n",
        "        \n",
        "        \n",
        "        ###### backpropagation and optimize###\n",
        "\n",
        "        model.zero_grad()\n",
        "        current_loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        #######################################\n",
        "        if batch_idx % 100 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100 * batch_idx / len(train_loader), current_loss.item()))\n",
        "    return(current_loss)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y1GRqdmVEdBf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import plotly"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u0GTlT1FUn4_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Define Testing Loop\n",
        "\n",
        "* Loop over batches of samples in the testing set\n",
        "* Run each batch through the model (forward pass)\n",
        "* Compute the loss and accuracy\n",
        "* Do not compute gradients or update model parameters \n",
        "* We are saving the testing data to evaluate how the model is doing on data it has not been trained on"
      ]
    },
    {
      "metadata": {
        "id": "GoQH-TLbUn4_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    # In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "    with torch.no_grad():\n",
        "      for data, target in test_loader:\n",
        "          # Move tensors to the configured device\n",
        "          data = data.to(device)\n",
        "          target = target.to(device)\n",
        "          #############Forward pass#############\n",
        "          output = model(data)\n",
        "          output = output.double\n",
        "          ######################################\n",
        "          test_loss += F.mse_loss(output, target, size_average=False).item() # sum up batch loss                                                               \n",
        "          pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability                                                                 \n",
        "          correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100 * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qB6HIUE5Un5B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Initialize the Model and Optimizer"
      ]
    },
    {
      "metadata": {
        "id": "9yV5RPTTUn5D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper parameters\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "\n",
        "#Initiate the model object using the class we've already defined\n",
        "num_classes = 1\n",
        "model = ConvNet(num_classes)\n",
        "\n",
        "\n",
        "#Move the model object to the Device\n",
        "model = model.to(device)\n",
        "\n",
        "#choose your desired optimizer##\n",
        "optimizer = optim.SGD(model.parameters(), learning_rate, momentum = 0.5) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7Q3s1umE4rAz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip freeze"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qAAG1r2T5I65",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7652eb21-48eb-4ebd-e79d-3902c2f91ff2"
      },
      "cell_type": "code",
      "source": [
        "!python3 -c 'import torch; print(torch); print(torch.__path__)'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<module 'torch' from '/usr/local/lib/python3.6/dist-packages/torch/__init__.py'>\n",
            "['/usr/local/lib/python3.6/dist-packages/torch']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NcNDZrpg7cUj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HSJdMOvzUn5E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train the Model\n",
        "\n",
        "* We will only train for a few epochs here\n",
        "* Normally we would train for longer\n",
        "* Depending on the dataset and model size, this can take days or weeks"
      ]
    },
    {
      "metadata": {
        "id": "w9a0oJTVUn5F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "outputId": "ef122642-e288-47fb-b887-588fa88b2176"
      },
      "cell_type": "code",
      "source": [
        "#Use a for loop to call train function and testing function\n",
        "#In this way, the model can be trained and tested in few epoches\n",
        "#######################################################\n",
        "import matplotlib.pyplot as plt\n",
        "num_epochs = 10\n",
        "epochs = []\n",
        "losses = []\n",
        "\n",
        "for i in range(num_epochs):\n",
        "  #train(i)\n",
        "  loss = train(i)\n",
        "  print(\"loss \" , loss)\n",
        "  print(i)\n",
        "  epochs.append(i)\n",
        "  losses.append(loss)\n",
        "  \n",
        "  test()\n",
        "plt.plot(epochs, losses)           \n",
        "#######################################################"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/7246 (0%)]\tLoss: 0.412578\n",
            "Train Epoch: 0 [500/7246 (7%)]\tLoss: 0.061782\n",
            "Train Epoch: 0 [1000/7246 (14%)]\tLoss: 0.266654\n",
            "Train Epoch: 0 [1500/7246 (21%)]\tLoss: 0.571561\n",
            "Train Epoch: 0 [2000/7246 (28%)]\tLoss: 0.379372\n",
            "Train Epoch: 0 [2500/7246 (34%)]\tLoss: 0.198860\n",
            "Train Epoch: 0 [3000/7246 (41%)]\tLoss: 0.268807\n",
            "Train Epoch: 0 [3500/7246 (48%)]\tLoss: 0.300486\n",
            "Train Epoch: 0 [4000/7246 (55%)]\tLoss: 0.497002\n",
            "Train Epoch: 0 [4500/7246 (62%)]\tLoss: 0.241866\n",
            "Train Epoch: 0 [5000/7246 (69%)]\tLoss: 0.231271\n",
            "Train Epoch: 0 [5500/7246 (76%)]\tLoss: 0.301036\n",
            "Train Epoch: 0 [6000/7246 (83%)]\tLoss: 0.178813\n",
            "Train Epoch: 0 [6500/7246 (90%)]\tLoss: 0.378770\n",
            "Train Epoch: 0 [7000/7246 (97%)]\tLoss: 0.079951\n",
            "loss  tensor(1.2809, device='cuda:0')\n",
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-5f9ee55f1794>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#######################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-88-39f00b852b29>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m           \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m           \u001b[0;31m######################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m           \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# sum up batch loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m           \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# get the index of the max log-probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m           \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce)\u001b[0m\n\u001b[1;32m   1567\u001b[0m     \"\"\"\n\u001b[1;32m   1568\u001b[0m     return _pointwise_loss(lambda a, b: (a - b) ** 2, torch._C._nn.mse_loss,\n\u001b[0;32m-> 1569\u001b[0;31m                            input, target, size_average, reduce)\n\u001b[0m\u001b[1;32m   1570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_pointwise_loss\u001b[0;34m(lambd, lambd_optimized, input, target, size_average, reduce)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1537\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlambd_optimized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: mse_loss(): argument 'input' (position 1) must be Tensor, not builtin_function_or_method"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "i5-9A4IVNDc6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "-"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "boIsksUoUn5G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Now show some model predictions on testing data\n",
        "\n",
        "* We will show an image from the testing set, and the probabilities the model assigns to each class"
      ]
    },
    {
      "metadata": {
        "id": "xdKWGluzUn5H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# show an image and the predicted probabilities                                                                                                               \n",
        "def display(i):\n",
        "    plt.figure(i + 1)\n",
        "    image, _ = test_loader.dataset.__getitem__(i)\n",
        "    image = image.reshape(1, 1, 28, 28).to(device)\n",
        "    output = model(image)\n",
        "    prob = F.softmax(output).squeeze().data\n",
        "    plt.subplot(121)\n",
        "    plt.imshow(image.squeeze().cpu().numpy())\n",
        "    plt.subplot(122)\n",
        "    plt.bar(range(10), prob.cpu().numpy())\n",
        "    plt.xlabel('predicted class probabilities')\n",
        "    plt.xticks(range(10))\n",
        "    plt.ylim([0, 1])\n",
        "\n",
        "for i in range(10):\n",
        "    display(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z5LqwliOUn5Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Things to try out\n",
        "\n",
        "* Try different numbers of layers and hidden units\n",
        "* Try different non-linearities (tanh, sigmoid)\n",
        "* Try on other datasets (CIFAR 10)"
      ]
    },
    {
      "metadata": {
        "id": "yGmZit-7hVg3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super(ConvNet, self).__init__()\n",
        "    self.layer1 = nn.Sequential(\n",
        "    nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=0),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "    self.layer2 = nn.Sequential(\n",
        "    nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=0),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "    self.fc = nn.Linear(4*4*32, num_classes)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out = self.layer1(x)\n",
        "    out = self.layer2(out)\n",
        "    out = out.reshape(out.size(0), -1)\n",
        "    out = self.fc(out)\n",
        "    return out\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}